{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a91a80-881d-4b0b-85de-0e3856b7768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "############################################\n",
    "# #!pip install  pillow\n",
    "\n",
    "############################################\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sys import getsizeof\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster , metrics\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import manifold, decomposition\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import time\n",
    "\n",
    "############################################\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "sns.set()\n",
    "\n",
    "###########################################\n",
    "import PIL\n",
    "from PIL import ImageFilter\n",
    "import keras\n",
    "\n",
    "###########################################\n",
    "#!pip install opencv-python\n",
    "import cv2\n",
    "\n",
    "##########################################\n",
    "PATH_IMAGE =  \"/media/brice_kengni_zanguim/Samsung_T5/Téléchargements/photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd0695d-af18-4f5f-8c5d-69af1d7c1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nested_value(d, key):\n",
    "    \"\"\"Return a dictionary item given a dictionary `d` and a flattened key from `get_column_names`.\n",
    "    Example:\n",
    "        d = {\n",
    "            'a': {\n",
    "                'b': 2,\n",
    "                'c': 3,\n",
    "                },\n",
    "        }\n",
    "        key = 'a.b'\n",
    "        will return: 2\n",
    "    \"\"\"\n",
    "    if '.' not in key:\n",
    "        if key not in d:\n",
    "            return None\n",
    "        return d[key]\n",
    "    base_key, sub_key = key.split('.', 1)\n",
    "    if base_key not in d:\n",
    "        return None\n",
    "    sub_dict = d[base_key]\n",
    "    return get_nested_value(sub_dict, sub_key)\n",
    "\n",
    "def get_row(line_contents, column_names):\n",
    "    \"\"\"Return a csv compatible row given column names and a dict.\"\"\"\n",
    "    row = []\n",
    "    for column_name in column_names:\n",
    "        line_value = get_nested_value(\n",
    "                        line_contents,\n",
    "                        column_name,\n",
    "                        )\n",
    "        if isinstance(line_value, bytes):\n",
    "            row.append('{0}'.format(line_value.encode('utf-8')))\n",
    "        elif isinstance( line_value , (float , int) ) :\n",
    "            row.append('{0}'.format(line_value))\n",
    "        elif line_value is not None:\n",
    "            row.append('{0}'.format(line_value.encode('utf-8')))\n",
    "        else:\n",
    "            row.append('')\n",
    "    \n",
    "    #row = [ bytes(i,\"UTF-8\") for i in row ]\n",
    "    return row\n",
    "\n",
    "def read_and_write_file(json_file_path, csv_file_path, column_names):\n",
    "    \"\"\"Read in the json dataset file and write it out to a csv file, given the column names.\"\"\"\n",
    "    \n",
    "    # convert the variable \"column_name\" to a byte type\n",
    "    #column_names = [ bytes(i,\"UTF-8\") for i in column_names ]\n",
    "    \n",
    "    with open(csv_file_path, 'w+') as fout:  #  wb+\n",
    "        csv_file = csv.writer(fout)\n",
    "        csv_file.writerow(column_names)\n",
    "        with open(json_file_path,'r', encoding=\"UTF-8\", errors='surrogateescape') as fin :\n",
    "            for line in fin:\n",
    "                line_contents = json.loads(line)\n",
    "                if isinstance( line_contents, bytes): line_contents = line_contents.decode()\n",
    "                                    \n",
    "                csv_file.writerow(get_row(line_contents, column_names))\n",
    "        \n",
    "def get_superset_of_column_names_from_file(json_file_path):\n",
    "    \"\"\"Read in the json dataset file and return the superset of column names.\"\"\"\n",
    "    column_names = set()\n",
    "    with open(json_file_path , encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            line_contents = json.loads(line)\n",
    "            column_names.update( set( get_column_names(line_contents).keys()) )\n",
    "    #column_names = [ i.decode() for i in column_names ]\n",
    "    return column_names\n",
    "\n",
    "def get_column_names(line_contents, parent_key=''):\n",
    "    \"\"\"Return a list of flattened key names given a dict.\n",
    "    Example:\n",
    "        line_contents = {\n",
    "            'a': {\n",
    "                'b': 2,\n",
    "                'c': 3,\n",
    "                },\n",
    "        }\n",
    "        will return: ['a.b', 'a.c']\n",
    "    These will be the column names for the eventual csv file.\n",
    "    \"\"\"\n",
    "    column_names = []\n",
    "    for k, v in line_contents.items():\n",
    "        column_name = \"{0}.{1}\".format(parent_key, k) if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            column_names.extend(\n",
    "                    get_column_names(v, column_name).items()\n",
    "                    )\n",
    "        else:\n",
    "            column_names.append((column_name, v))\n",
    "    return dict(column_names)\n",
    "\n",
    "def sampling_data( file_name_path = None, datas =None , proportion= 1 ) :\n",
    "    if file_name_path :\n",
    "        n = sum(1 for line in open(file_name_path))-1\n",
    "        s = int(n*proportion/100)  \n",
    "        skip = sorted(random.sample(range(1, n+1), n-s))\n",
    "        \n",
    "        return pd.read_csv(file_name_path, skiprows=skip)\n",
    "    else :\n",
    "        data = datas.copy()\n",
    "        n = data.shape[0]\n",
    "        s = int(n*proportion/100)  \n",
    "        skip = sorted(random.sample(list(data.index) ,n-s))\n",
    "        \n",
    "        return data.drop( index = skip )\n",
    "        \n",
    "def bytes_to_str( data , columns ) :\n",
    "    data = data.copy()\n",
    "    for col in columns :\n",
    "        for idx in data.index :\n",
    "            data.loc[idx , col] = data.loc[idx , col][2:-1]\n",
    "    return data\n",
    "\n",
    "def document_frequence ( texts ) :\n",
    "    \"\"\" Renvoie la frequence des mots/tokens dans un text/document\"\"\"\n",
    "    \n",
    "    if type(texts) != list :\n",
    "        texts = nltk.tokenize.word_tokenize(texts)\n",
    "\n",
    "    return nltk.FreqDist(  texts ) \n",
    "\n",
    "def corpus_frequence ( data , col) :\n",
    "    \"\"\" Renvoie la frequence des mots/tokens dans tout le corpus\"\"\"\n",
    "    data = data.copy()\n",
    "    \n",
    "    # Vérifications sur les entrées\n",
    "    if col not in data.columns :\n",
    "        print( f\"La feature `{col}` que vous avez fournie n'est pas valide\" )\n",
    "        return\n",
    "    \n",
    "    freq_tot = nltk.Counter()\n",
    "    for text in data[col] :\n",
    "        freq_tot += document_frequence(texts = text)\n",
    "        \n",
    "    return freq_tot\n",
    "\n",
    "def traitement_de_texte(data, col, langage=\"english\", to_minuscule=True, rm_ponctuation=True, rm_number =True, tokenize_sentence=True, \\\n",
    "                        rm_stop_words=True, lemmatize_sentence= True, stemming_sentence=True, remove_most_frequent = (False , 200) ,\\\n",
    "                        remove_less_frequent = (True , 10) ,token_min_len = 3, complete_stopworld =True ) :\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    # Vérifications sur les entrées\n",
    "    if col not in data.columns :\n",
    "        print( f\"La feature `{col}` que vous avez fournie n'est pas valide\" )\n",
    "        return\n",
    "    if langage not in [\"english\", \"french\"] :\n",
    "        print(f\"la variable `langage` prend uniquement les valeurs `english` ou `french`\")\n",
    "        return\n",
    "    \n",
    "    # Utilitaires NLTK\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    if langage == \"english\" :\n",
    "        stemer_obj = nltk.PorterStemmer()\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        nltk.download('words')\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    else :\n",
    "        #!pip install FrenchLefffLemmatizer\n",
    "        #!pip install \"E:\\Mon bureau\\Mes investissements\\Openclassroom\\Formation Ingénieur IA\\Projet 6\\P6_Brice_KENGNI_ZANGUIM\\FrenchLefffLemmatizer\"\n",
    "        from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "        words = set(line.strip() for line in open('dictionnaire_Fr.txt',\"r+\", encoding=\"UTF-8\"))\n",
    "        stemer_obj = nltk.stem.snowball.FrenchStemmer()\n",
    "        lemmatizer = FrenchLefffLemmatizer()\n",
    "    \n",
    "    \n",
    "    stopwords = set()\n",
    "    stopwords.update(nltk.corpus.stopwords.words(langage))\n",
    "    if langage == \"english\" and complete_stopworld : stopwords.update([\"in\",\"then\",\"the\",\"then\",\"that\",\"this\",\"there\",\"we\",\"of\",\"they\",\"but\",\"where\",\"so\",\"only\",\"got\",\\\n",
    "                                               \"was\",\"say\",\"use\",\"away\",\"need\",\"get\",\"line\",\"want\",\"one\",\"two\",\"give\",\"see\",\"came\",\"let\",\"way\",\n",
    "                                                \"come\",\"also\",\"going\",\"done\",\"put\",\"got\",\"went\",\"could\",\"dont\",\"didnt\",\"first\",\"since\",\"made\",\\\n",
    "                                               \"wasnt\",\"said\",\"would\",\"make\",\"take\",\"every\",\"else\",\"told\",\"even\",\"eat\",\"new\",\"tell\",\"know\",\"thing\",\\\n",
    "                                                \"thats\",\"used\"])\n",
    "    \n",
    "    # Passage du texte en minuscule\n",
    "    if to_minuscule : data[col] = data[col].apply(lambda sentence : sentence.lower())\n",
    "    # Gestion de la ponctuation\n",
    "    if rm_ponctuation : data[col] = data[col].apply(lambda sentence : \"\".join( [i for i in sentence if i not in string.punctuation] ) )\n",
    "    # Suppression des chiffres\n",
    "    if rm_number : data[col] = data[col].apply(lambda sentence : \"\".join(i for i in sentence if not i.isdigit() ) )\n",
    "    # Tokenisation\n",
    "    if tokenize_sentence : data[col] = data[col].apply(lambda sentence : nltk.tokenize.word_tokenize(sentence))\n",
    "    # Suppression des stopwords\n",
    "    if rm_stop_words : data[col] = data[col].apply(lambda sentence : [i for i in sentence if i not in stopwords] )\n",
    "    # Lemmatisation\n",
    "    if lemmatize_sentence : \n",
    "        data[col] = data[col].apply(lambda sentence : [lemmatizer.lemmatize(w) for w in sentence ] )\n",
    "    elif stemming_sentence :\n",
    "        # Stemming\n",
    "        data[col] = data[col].apply(lambda sentence : [stemer_obj.stem(w) for w in sentence ] )\n",
    "    # Suppression de token de taille inférieure à  `token_min_len`\n",
    "    data[col] = data[col].apply(lambda sentence : [ i for i in sentence if len(i) >= token_min_len])    \n",
    "    \n",
    "    # Ajouter les mots les plus frequents dans la liste de stopwords et suppression du corpus\n",
    "    corpus_freq = corpus_frequence(data , col)\n",
    "    if remove_most_frequent[0] :\n",
    "        mc = list(zip(*corpus_freq.most_common( remove_most_frequent[1] ) ))[0] \n",
    "        # Suppression\n",
    "        data[col] = data[col].apply(lambda sentence : [i for i in sentence if i not in mc] )\n",
    "    # Supprimer les mots les plus rares\n",
    "    if remove_less_frequent[0] : \n",
    "        serie_freq = pd.Series(corpus_freq)\n",
    "        lc = np.array( serie_freq.index[ serie_freq <= remove_less_frequent[1]*data.shape[0]/100 ] )\n",
    "        # Suppression \n",
    "        data[col] = data[col].apply(lambda sentence : [i for i in sentence if i not in lc] )\n",
    "    \n",
    "    # Nombre de Tokens et nombre de tokens uniques\n",
    "    data[\"Nb_Tokens_uniques\"] = data[col].apply( lambda sentence : np.unique(sentence).size ) \n",
    "    data[\"Nb_Tokens\"] = data[col].apply( lambda sentence : len(sentence))\n",
    "    # Reconstruction des phrases \n",
    "    data[col] = data[col].apply(lambda sentence : \" \".join(w for w in sentence if w in words or not w.isalpha()) )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def hyerarchical_clustering ( data ,link_criterios = \"ward\", profondeur = 3 ) :\n",
    "    data = data.copy()\n",
    "    linked = linkage( data , link_criterios )\n",
    "    labelList = data.columns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram( linked , truncate_mode = 'level' , p=profondeur , orientation='top', distance_sort='descending', show_leaf_counts = True  )\n",
    "    plt.show()\n",
    "\n",
    "def World_cloud_show( data , fig_size = (8,5), max_wd = 150, do_mask = True ) :\n",
    "    if do_mask : \n",
    "        mask = np.array(Image.open(\"cloud.jpg\"))\n",
    "        mask[mask > 1] = 255\n",
    "        x = wordcloud.WordCloud(width = fig_size[0]*100, height = fig_size[1]*100, background_color ='white',colormap=\"plasma\", min_font_size=3,\\\n",
    "                            max_words=max_wd, mask = mask).generate(\" \".join( data.text))\n",
    "    else :\n",
    "        x = wordcloud.WordCloud(width = fig_size[0]*100, height = fig_size[1]*100, background_color ='white',colormap=\"plasma\", min_font_size=3,\\\n",
    "                            max_words=max_wd, ).generate(\" \".join( data.text))\n",
    "\n",
    "    # plot the WordCloud image                      \n",
    "    plt.figure(figsize = fig_size, facecolor = None)\n",
    "    plt.imshow(x)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.show()\n",
    "\n",
    "def TSNE_plot (data , clustering_model , method='exact' , transp = 0.4, perplx = 10) :\n",
    "    data = data.copy()\n",
    "    ########## COULEURS\n",
    "\n",
    "    labl_hc = clustering_model.labels_\n",
    "    couleurs  = np.random.choice( [ \"red\", \"orange\", \"green\", \"blue\", \"blueviolet\",\"black\",\"brown\" ,\"navy\" ,\"purple\", \"magenta\", \"gold\"] ,\\\n",
    "                                 np.unique( labl_hc ).size , replace = False )\n",
    "    \n",
    "    couleur_hc = pd.Series(labl_hc).apply( lambda x : couleurs[x])\n",
    "\n",
    "    tsne_hc = sklearn.manifold.TSNE(n_components = 3, perplexity = perplx, n_iter = 1500, n_iter_without_progress = 200, init = 'pca', \\\n",
    "                                    n_jobs= os.cpu_count() , method= method)\n",
    "    tsne_hc.fit( data )\n",
    "\n",
    "    fig , axes = plt.subplots( 1, 3 ,figsize = (22, 7) )\n",
    "    plt.title(\"XY\" , size = 15)\n",
    "    sns.scatterplot(tsne_hc.embedding_[:,0], tsne_hc.embedding_[:,1],  c = couleur_hc, alpha=transp, s=120, ax=axes[0] )\n",
    "    plt.title(\"XZ\" , size = 15)\n",
    "    sns.scatterplot(tsne_hc.embedding_[:,0], tsne_hc.embedding_[:,2],  c = couleur_hc, alpha=transp, s=120, ax=axes[1] )\n",
    "    plt.title(\"YZ\" , size = 15)\n",
    "    sns.scatterplot(tsne_hc.embedding_[:,1], tsne_hc.embedding_[:,2],  c = couleur_hc, alpha=transp, s=120, ax=axes[2] )\n",
    "\n",
    "def clustering_model_choice ( data , n_clus = 2, modl = \"km\") :\n",
    "    data = data.copy()\n",
    "    if modl == \"km\" :\n",
    "        model = sklearn.cluster.KMeans(n_clusters=n_clus,  max_iter=400 , n_init= 20)   \n",
    "    elif modl == \"hc\" :\n",
    "        model = AgglomerativeClustering(n_clusters=n_clus, affinity='euclidean', linkage='ward')\n",
    "    model.fit( data )\n",
    "    return model\n",
    "\n",
    "def show_feat_get_descrip ( image, n_feat = 300 , show_feat = True) :\n",
    "    image = np.asarray( image )\n",
    "    sift = cv2.SIFT_create(n_feat )\n",
    "    kp, des = sift.detectAndCompute( image  , None)\n",
    "    if show_feat :\n",
    "        img=cv2.drawKeypoints( image ,kp,  image  )\n",
    "        plt.figure(figsize=(9,6))\n",
    "        plt.imshow(img)\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return des\n",
    "\n",
    "def histogram_d_image( descripteurs_toutes_les_images, clustering_model = \"KMEANS\" , prop_clust = 1.3):\n",
    "    # Nombre de clusters des descripteurs\n",
    "    nombre_de_cluster_features = int(prop_clust*np.sqrt( np.concatenate( descripteurs_toutes_les_images  ).shape[0] ) )\n",
    "    \n",
    "    if clustering_model == \"KMEANS\" : \n",
    "        clustering_model = cluster.MiniBatchKMeans(n_clusters = nombre_de_cluster_features, init_size = 2*nombre_de_cluster_features,  max_iter = 200, n_init = 40)\n",
    "        clustering_model.fit(  np.concatenate( descripteurs_toutes_les_images  )  )\n",
    "    \n",
    "    # Matrice des histogrammes des images: shape = (  Nombre d'images , Nombres de clusters )\n",
    "    hist_vectors=[]\n",
    "    #t_i = time.time()\n",
    "    for num , img_descrip in enumerate( descripteurs_toutes_les_images ) :\n",
    "        hist = np.zeros(len(clustering_model.cluster_centers_))\n",
    "        # Prédiction des clusters de chaque\n",
    "        clusters_des_descripteurs = clustering_model.predict(img_descrip)\n",
    "    \n",
    "        if len(img_descrip) == 0 :\n",
    "            print(f\"L'image numéro {num} n'a pas de descriptieur\")\n",
    "            continue\n",
    "        else :\n",
    "            for numero_de_cluster in clusters_des_descripteurs:\n",
    "                hist[ numero_de_cluster ] += 1.0\n",
    "        hist_vectors.append( hist/len(img_descrip) )\n",
    "        #print(time.time() - t_i)\n",
    "    return np.asarray( hist_vectors )\n",
    "\n",
    "def img_show( imag ): \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(imag)\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def img_hist( img , cumul = False ): \n",
    "    plt.figure(figsize=(7,5))\n",
    "    sns.histplot( np.array( img ).ravel(), cumulative= cumul, color='blue')\n",
    "    plt.xlabel(\"Niveau de gris\", size=15)\n",
    "    plt.show()\n",
    "\n",
    "def img_repixels ( img , Min = 0 , Max = 255 , auto = False):\n",
    "    if auto :\n",
    "        return  PIL.ImageOps.autocontrast(img)\n",
    "    else :\n",
    "        mat = np.array(img).astype(int)\n",
    "\n",
    "        if mat.shape[2] >= 2 :\n",
    "            for i in range( mat.shape[2] ) :\n",
    "                mat[:,:,i] = Min + (mat[:,:,i] - mat[:,:,i].min())*( Max - Min )/( mat[:,:,i].max() - mat[:,:,i].min())\n",
    "        else :\n",
    "            mat = Min + (mat - mat.min())*( Max - Min )/( mat.max() - mat.min())\n",
    "\n",
    "        return PIL.Image.fromarray(  mat.astype('uint8')  )\n",
    "    \n",
    "\n",
    "def get_train_test_index( data , label = \"label\", nombre=(200,200) ) :\n",
    "    data = data.copy()\n",
    "    lignes_train, lignes_test = [], []\n",
    "    \n",
    "    if np.asarray(nombre).size == 1 :\n",
    "        nombre = np.asarray(nombre)\n",
    "        if nombre < 1. :\n",
    "            nombre = nombre*data.shape[0]\n",
    "            nombre = nombre.astype(int)\n",
    "\n",
    "        lignes_train =  random.sample( sorted(data.index.values), nombre )\n",
    "        lignes_test = data.drop(index = lignes_train ).index.values\n",
    "        \n",
    "    elif np.asarray(nombre).size == 2 :\n",
    "        for cat in data[label].unique() : \n",
    "            sub_data = data[ data[label] == cat]\n",
    "            for i in random.sample( sorted(sub_data.index.values), nombre[0] ) :\n",
    "                lignes_train.append(i)\n",
    "\n",
    "        data.drop(index = lignes_train , inplace =True)\n",
    "        for cat in data[label].unique() : \n",
    "            sub_data = data[ data[label] == cat]\n",
    "            for i in random.sample( sorted(sub_data.index.values), nombre[1] ) :\n",
    "                lignes_test.append(i)\n",
    "    \n",
    "    return sorted( lignes_train ) , sorted( lignes_test )\n",
    "\n",
    "\n",
    "def load_images ( data, label=\"photo_id\", path = PATH_IMAGE ) :\n",
    "    data = data.copy()\n",
    "    data[\"images\"] = np.nan\n",
    "\n",
    "    for idx in data.index :\n",
    "        try :\n",
    "            # Ouvertures des photos\n",
    "            data[\"images\"][idx]  = PIL.Image.open( f\"{path}/{data.loc[idx,label]}\" ) \n",
    "        except :\n",
    "            continue\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def process_images ( data , label=\"images\" , process_type = \"RGEC\", resolution =( 300,300),gauss_size=2): \n",
    "    data = data.copy()\n",
    "    \n",
    "    if \"E\" in process_type :\n",
    "        # Uniformisation de la distributuion de pixels\n",
    "        data[\"images_processes\"] = data[label].apply(lambda image : PIL.ImageOps.equalize(PIL.ImageOps.equalize(image ) ) ) \n",
    "    \n",
    "    if \"G\" in process_type :\n",
    "        #picts[\"images_process\"] = picts.images_process.apply(lambda image : img_repixels( image,  20, 220) )\n",
    "        data[\"images_processes\"] = data.images_processes.apply(lambda image : image.filter( PIL.ImageFilter.GaussianBlur(gauss_size) )  )\n",
    "    if \"C\" in process_type :\n",
    "        # Mise en contraste de l'image\n",
    "        data[\"images_processes\"] = data.images_processes.apply(lambda image : PIL.ImageOps.autocontrast( image )  )\n",
    "    \n",
    "    if \"R\" in process_type :\n",
    "        data[\"images_processes\"] = data.images_processes.apply( lambda img : np.asarray(img) )\n",
    "        data['images_processes'] = data.images_processes.apply( lambda img :  cv2.resize(img , resolution, interpolation=cv2.INTER_AREA ) )\n",
    "    \n",
    "    data['images_process'] = data[\"images_processes\"]\n",
    "    data.drop( columns=[\"images_processes\"] ,inplace = True)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367a31e-b469-4a90-aadb-2c74f58ab274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
