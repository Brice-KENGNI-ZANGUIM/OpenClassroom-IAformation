{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a91a80-881d-4b0b-85de-0e3856b7768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "############################################\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import keras\n",
    "import os\n",
    "import wordcloud\n",
    "from PIL import Image\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn import preprocessing, manifold, metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import nltk\n",
    "\n",
    "############################################\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "sns.set()\n",
    "\n",
    "###########################################\n",
    "#!pip3 install --upgrade tensorflow-gpu\n",
    "# Install TF-Hub.\n",
    "#!pip3 install tensorflow-hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd0695d-af18-4f5f-8c5d-69af1d7c1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "class ToLowerCase(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        return X.apply(lambda tweet : tweet.lower() )\n",
    "\n",
    "class RemoveUserName(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        return X.apply( lambda tweet : \" \".join( token.text for token in nlp(tweet) if \"@\" != token.text[0]) )\n",
    "\n",
    "class Lemmatization(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        return X.apply( lambda tweet : \" \".join( token.lemma_ for token in nlp(tweet)) )\n",
    "\n",
    "class Stemmatization(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        stemmer = nltk.PorterStemmer()\n",
    "        return X.apply( lambda tweet : \" \".join( stemmer.stem(token.text) for token in nlp(tweet)) )\n",
    "\n",
    "class StopWord(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        stpwd = [\":|\",\":\",\"|\",\"ã\",\"½\",\"¿\",\"\",\"iãâ¯ãâ¿ãâ½in\",\"¯\",\"canãâ¯ãâ¿ãâ½t\",\"=/\",\":-p\",\":p\",\"-p\",\"/\",\"=\",\":|\",\"ãâ¯ãâ¿ãâ½i\",\n",
    "                 \"thãâ¯ãâ¿ãâ½n\",\"khãâ¯ãâ¿ãâ½m\",\"ãâ¯ãâ¿ãâ½\",\"â\"]\n",
    "        return X.apply( lambda tweet : \" \".join( token.text for token in nlp(tweet) if not token.is_stop and token.text not in stpwd ) )\n",
    "    \n",
    "class Ponctuation(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        return X.apply( lambda tweet : \" \".join( token.text for token in nlp(tweet) if not token.is_punct ) )\n",
    "    \n",
    "class RemoveSpace(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        return X.apply( lambda tweet : \" \".join( token.text for token in nlp(tweet) if not token.is_space ) )\n",
    "    \n",
    "class RemoveURL(BaseEstimator ,TransformerMixin ): \n",
    "    def fit ( self , X , y = None ): \n",
    "        return self\n",
    "    \n",
    "    def transform ( self , X) :\n",
    "        return X.apply( lambda tweet : \" \".join(token.text for token in nlp(tweet) if not token.like_url ))\n",
    "\n",
    "def document_encoding_algo ( datas, col , model_encoding = \"USE\",n_gram=(1,1), min_df = 0.001 , max_df = 1. , vocabulaire=None, get_vocabulary=False) :\n",
    "    data = datas.copy()\n",
    "    \n",
    "    if model_encoding == \"USE\" :\n",
    "        module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
    "        model_use = hub.load(module_url)\n",
    "        \n",
    "        return pd.DataFrame.from_dict( {  f\"{k}\": np.array(model_use( [data[col][k]] )).reshape(512,) for k in data.index  } , orient='index' )\n",
    "    elif model_encoding == \"TFIDF\" :\n",
    "        \n",
    "        tfidf_vect = TfidfVectorizer( ngram_range=n_gram, min_df = min_df , max_df = max_df, vocabulary = vocabulaire )\n",
    "        tfIdf = tfidf_vect.fit_transform(data[col]).toarray()\n",
    "        \n",
    "        if not get_vocabulary :\n",
    "            return pd.DataFrame(tfIdf, columns= tfidf_vect.get_feature_names())\n",
    "        else :\n",
    "            return tfidf_vect.vocabulary_\n",
    "\n",
    "    elif model_encoding == \"countvectorizer\" :\n",
    "        count_vect = CountVectorizer(ngram_range=n_gram, min_df = min_df , max_df = max_df, vocabulary = vocabulaire )\n",
    "        out = count_vect.fit_transform(data[col]).toarray()\n",
    "        \n",
    "        if not get_vocabulary :\n",
    "            return pd.DataFrame( out , columns=count_vect.get_feature_names())\n",
    "        else :\n",
    "            return count_vect.vocabulary_\n",
    "    else :\n",
    "        raise ValueError(f\"Le modèle d'encodage \\\"{model_encoding}\\\" n'est pas disponible\")\n",
    "\n",
    "def TSNE_plot (data , clustering_model , method='exact' , transp = 0.4, perplx = 10) :\n",
    "    data = data.copy()\n",
    "    ########## COULEURS\n",
    "\n",
    "    labl_hc = clustering_model.labels_\n",
    "    couleurs  = np.random.choice( [ \"red\", \"orange\", \"green\", \"blue\", \"blueviolet\",\"black\",\"brown\" ,\"navy\" ,\"purple\", \"magenta\", \"gold\"] ,\\\n",
    "                                 np.unique( labl_hc ).size , replace = False )\n",
    "    \n",
    "    couleur_hc = pd.Series(labl_hc).apply( lambda x : couleurs[x])\n",
    "\n",
    "    tsne_hc = sklearn.manifold.TSNE(n_components = 3, perplexity = perplx, n_iter = 1500, n_iter_without_progress = 200, init = 'pca', \\\n",
    "                                    n_jobs= os.cpu_count() , method= method)\n",
    "    tsne_hc.fit( data )\n",
    "\n",
    "    fig , axes = plt.subplots( 1, 3 ,figsize = (22, 7) )\n",
    "    plt.title(\"XY\" , size = 15)\n",
    "    sns.scatterplot(tsne_hc.embedding_[:,0], tsne_hc.embedding_[:,1],  c = couleur_hc, alpha=transp, s=120, ax=axes[0] )\n",
    "    plt.title(\"XZ\" , size = 15)\n",
    "    sns.scatterplot(tsne_hc.embedding_[:,0], tsne_hc.embedding_[:,2],  c = couleur_hc, alpha=transp, s=120, ax=axes[1] )\n",
    "    plt.title(\"YZ\" , size = 15)\n",
    "    sns.scatterplot(tsne_hc.embedding_[:,1], tsne_hc.embedding_[:,2],  c = couleur_hc, alpha=transp, s=120, ax=axes[2] )\n",
    "\n",
    "def plot_confusion_matrix(Y_true , Y_predict, title=\"Matrice de confusion\", cmap=\"hot_r\", figsize = (12,8), tic_rot= (0,0)):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        warnings.filterwarnings('ignore')\n",
    "        Y_true , Y_predict = np.asarray(Y_true), np.asarray(Y_predict)\n",
    "        df_confusion = pd.crosstab(Y_true, Y_predict, rownames=['True Labels\\n'], colnames=['\\nPredicted Labels'])\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(df_confusion, cmap=cmap, annot =True) # imshow\n",
    "        plt.title(title,size =3*figsize[0])\n",
    "        #plt.colorbar()\n",
    "        tick_marks = np.arange(len(df_confusion.columns)) +0.5\n",
    "        plt.xticks(tick_marks, df_confusion.columns, rotation=tic_rot[0], size=1.5*figsize[0])\n",
    "        plt.yticks(tick_marks, df_confusion.index, rotation=tic_rot[1], size = 1.5*figsize[0])\n",
    "        #plt.tight_layout()\n",
    "        plt.ylabel(df_confusion.index.name, size = 2*figsize[0] )\n",
    "        plt.xlabel(df_confusion.columns.name, size = 2*figsize[0])\n",
    "        plt.show()\n",
    "\n",
    "def correspondance_entre_clusters ( True_labels , predict_labels ) :\n",
    "    clustering_label = np.asarray([ \"                     \" for i in predict_labels ])\n",
    "    for clus_class in np.unique( predict_labels ) :\n",
    "        sub = True_labels[ predict_labels == clus_class ]\n",
    "        lbl , count = np.unique( sub , return_counts = True )\n",
    "        clustering_label[predict_labels == clus_class] = f\"{lbl[np.argmax(count)]} => Cluster {clus_class}\"\n",
    "        \n",
    "    return clustering_label\n",
    "\n",
    "def get_train_test_index( data , label = \"label\", nombre=(200,200,200) ) :\n",
    "    data = data.copy()\n",
    "    \n",
    "    idx = [[] for i in range(len(nombre)) ]\n",
    "    for i in range(len(nombre)) :\n",
    "        for cat in data[label].unique() : \n",
    "            sub_data = data[ data[label] == cat]\n",
    "            for j in random.sample( sorted(sub_data.index.values), nombre[i] ) :\n",
    "                idx[i].append(j)\n",
    "        data.drop(index=idx[i] , inplace=True)\n",
    "        idx[i] = sorted(idx[i])\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def prediction_function_threshold ( model=None , X=None, Y_proba=None , seuil = 0.5 ) :\n",
    "    \"\"\"\n",
    "    La fonction permet d'évaluer la prediction d'un modèle donnée en fonction du modèle, de l'entrée, d'une probabilité fournie et d'un seuil \n",
    "    \n",
    "    Paramètres :\n",
    "    ------------\n",
    "        model : modèle de machine learning à utiliser.\n",
    "        X : pandas.core.frame.DataFrame\n",
    "            donnée à fournir au modèle pour la prédiction. Si model est fourni alors X doit aussi être fourni et dans ce cas Y_proba n'est pas utilisé.\n",
    "        Y_proba : Array_type\n",
    "            probabilité qu'un individu soit du label positif. Si Y_proba est fourni alors model et X ne sont pas utilisés\n",
    "        seuil : float\n",
    "            seuil de probalité à utiliser pour la calcul de la prediction. la valeur par defaus est 0.5 et doit toujours être comprise entre 0 et 1\n",
    "    \n",
    "    Return : Array_type\n",
    "        prediction\n",
    "        \n",
    "    \"\"\"\n",
    "    if ( type(model) == type(None) ) and ( type(X) == type(None) ): \n",
    "        return np.array( Y_proba > seuil , dtype = int)\n",
    "    else :\n",
    "        try : \n",
    "            return np.array( model.predict_proba(X)[:,1] > seuil , dtype = int)\n",
    "        except :\n",
    "            print(\"Le modèle que vous avez fourni ne possède pas de méthode 'predict_proba()'\")\n",
    "            return\n",
    "        \n",
    "#  Definition de la meilleure métrique \n",
    "#  Definition de la meilleure métrique \n",
    "def my_cost( y , y_pred , poids = 4  , seuil = np.linspace(0.008,0.999,100 ) , scorer = False ) : \n",
    "    \"\"\"\n",
    "    La fonction permet d'évaluer le cout métier d'un modèle donné \n",
    "    \n",
    "    Paramètres :\n",
    "    ------------\n",
    "        y : Array_like \n",
    "            vraie valeur labels pour chaque individus ou observations\n",
    "        y_pred : Array_like \n",
    "            vecteur probabilité ( d'être positif  ) peu aussi être le vecteur prediction du modèle pour chaque individus ou observations\n",
    "            \n",
    "    return : dict\n",
    "    ------------\n",
    "        out : dict \n",
    "        La dictionnaire renvoyé est de la forme : \n",
    "        { \"cout\" : Array_type de variation du coût en fonction du seuil , \"cout_min\" : valeur du coût minimal , \"seuil_min\" : seuil correspondant au cout minimal }\n",
    "    \"\"\"\n",
    "    if type(y) == type(pd.DataFrame()) :\n",
    "        label = pd.DataFrame( {\"Y_test\" : y.values.reshape( (y.shape[0],) ) , \"Y_prob\" : y_pred } )\n",
    "    else :\n",
    "        label = pd.DataFrame( {\"Y_test\" : y , \"Y_prob\" : y_pred } )\n",
    "\n",
    "    out = {\"cout\" : []}\n",
    "    if type(seuil) in [ float , np.float16, np.float32, np.float64] : \n",
    "        label[\"Y_pred\"] = label[\"Y_prob\"].apply( lambda x : int(x > seuil))\n",
    "        label.loc[ (label[\"Y_test\"] ==0) & (label[\"Y_pred\"] == 0) , \"decision\"]  = \"VN\"\n",
    "        label.loc[ (label[\"Y_test\"] ==1) & (label[\"Y_pred\"] == 1) , \"decision\"]  = \"VP\"\n",
    "        label.loc[ (label[\"Y_test\"] ==0) & (label[\"Y_pred\"] == 1) , \"decision\"]  = \"FP\"\n",
    "        label.loc[ (label[\"Y_test\"] ==1) & (label[\"Y_pred\"] == 0) , \"decision\"]  = \"FN\"\n",
    "        return ( label[\"decision\"] == \"FP\" ).mean() + poids*( label[\"decision\"] == \"FN\" ).mean()\n",
    "    else :\n",
    "        for s in seuil : \n",
    "            label[\"Y_pred\"] = label[\"Y_prob\"].apply( lambda x : int(x > s))\n",
    "            label.loc[ (label[\"Y_test\"] ==0) & (label[\"Y_pred\"] == 0) , \"decision\"]  = \"VN\"\n",
    "            label.loc[ (label[\"Y_test\"] ==1) & (label[\"Y_pred\"] == 1) , \"decision\"]  = \"VP\"\n",
    "            label.loc[ (label[\"Y_test\"] ==0) & (label[\"Y_pred\"] == 1) , \"decision\"]  = \"FP\"\n",
    "            label.loc[ (label[\"Y_test\"] ==1) & (label[\"Y_pred\"] == 0) , \"decision\"]  = \"FN\"\n",
    "            out[\"cout\"].append( ( label[\"decision\"] == \"FP\" ).mean() + poids*( label[\"decision\"] == \"FN\" ).mean() )\n",
    "\n",
    "        out[\"cout\"] = np.array(out[\"cout\"])\n",
    "        if scorer : \n",
    "            return out[\"cout\"].min()\n",
    "        else : \n",
    "            out[\"cout_min\"] = out[\"cout\"].min()\n",
    "            out[\"seuil_min\"] = seuil[out[\"cout\"] == out[\"cout_min\"]]\n",
    "            out[\"cout\"] = list(out[\"cout\"])\n",
    "            return out\n",
    "        \n",
    "def print_scores(model = None , X_test=None , Y_true=None , Y_proba = None  , line_width = 6 , seuil = np.linspace( 0 , 0.9 , 90 ) ,\n",
    "                 plot_kind = \"apr\", give_results = False , show_graph = True, poids = None, fig_sz = (12,8)) :\n",
    "    scores = {} \n",
    "    beta = np.linspace( 0.7 , 2. , 2 )\n",
    "    if ( type(model)!=type(None) ) and ( type(X_test)!=type(None) ) and ( type(Y_proba) == type(None) ) :\n",
    "        try :\n",
    "            Y_proba = model.predict_proba(X_test)[:,1]\n",
    "        except :\n",
    "            print(\"Le modèle que vous avez fourni ne possède pas de méthode 'predict_proba'\")\n",
    "            return\n",
    "    \n",
    "    ## Pour chaque coefficient beta de F_beta score, je vais calculter la variation du F_beta score avec le seuil\n",
    "    if \"b\" in plot_kind :\n",
    "        for b in beta :\n",
    "            scores[f\"beta = {b}\"] = [  metrics.fbeta_score(Y_true , prediction_function_threshold(Y_proba = Y_proba ,seuil = s) , beta = b ) for s in seuil ]\n",
    "    if \"a\" in plot_kind :\n",
    "        scores[f\"Accuracy\"] = [ metrics.accuracy_score(Y_true , prediction_function_threshold(Y_proba = Y_proba ,seuil = s)  )  for s in seuil  ]      #  Accuracy en fonction du seuil\n",
    "    if \"r\" in plot_kind :\n",
    "        scores[f\"Recall\"] = [ metrics.recall_score(Y_true , prediction_function_threshold(Y_proba = Y_proba ,seuil = s)  ) for s in seuil  ]           #  Recall en fonction du seuil\n",
    "    if \"p\" in plot_kind :\n",
    "        scores[f\"Precision\"] = [ metrics.precision_score(Y_true , prediction_function_threshold(Y_proba = Y_proba ,seuil = s)  ) for s in seuil  ]     #  Precision en fonction du seuil\n",
    "    if \"h\" in plot_kind :\n",
    "        scores[f\"Hamming Loss\"] = [ metrics.hamming_loss(Y_true , prediction_function_threshold(Y_proba = Y_proba ,seuil = s)  ) for s in seuil  ]     #  Precision en fonction du seuil\n",
    "    if \"c\" in plot_kind :\n",
    "        scores[\"Fonction Coût\"] = my_cost(Y_true , Y_proba , seuil = seuil ,scorer=False, poids = poids)[\"cout\"]     #  Precision en fonction du seuil\n",
    "    \n",
    "    if show_graph :\n",
    "        #  Affichage de la figure\n",
    "        plt.figure(figsize = fig_sz)\n",
    "        plt.title(\"\\nF_beta-accuracy-Précision-Recall VS seuil\" , size=2*fig_sz[0])\n",
    "        plt.xlabel(\" Seuil de probabilité\" , size= 1.5*fig_sz[0])\n",
    "        plt.ylabel(\"SCORE\" , size = 1.5*fig_sz[0] )\n",
    "        \n",
    "        for label , y  in scores.items()  :\n",
    "            plt.plot(seuil , y , lw = line_width ,ls = np.random.choice([\"dashed\",\"dotted\", \"dashdot\", \"solid\"]), label = f\"{label}\")\n",
    "            \n",
    "        plt.legend(loc=\"best\" , fontsize=\"xx-large\")\n",
    "        plt.show()\n",
    "        scores[\"seuil\"] = seuil\n",
    "    if give_results : return pd.DataFrame(scores ).set_index(\"seuil\")\n",
    "\n",
    "def World_cloud_show( data , fig_size = (8,5), max_wd = 150, do_mask = True, horizontal= .85, min_font = 5, font_step= 2 ) :\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        if do_mask : \n",
    "            mask = np.array(Image.open(\"cloud.jpg\"))\n",
    "            mask[mask > 1] = 255\n",
    "            x = wordcloud.WordCloud(width = fig_size[0]*100, height = fig_size[1]*100, background_color ='white', colormap=\"plasma\", max_words=max_wd,\n",
    "                                    repeat = False, min_font_size=min_font, font_step=font_step, prefer_horizontal = horizontal, mask = mask,\n",
    "                                    relative_scaling =0, collocations =False).generate(\" \".join( data))\n",
    "        else :\n",
    "            x = wordcloud.WordCloud(width = fig_size[0]*100, height = fig_size[1]*100, background_color ='white', colormap=\"plasma\", max_words=max_wd,\n",
    "                                    repeat = False, min_font_size=min_font, font_step=font_step, prefer_horizontal = horizontal,relative_scaling =0,\n",
    "                                    collocations =False).generate(\" \".join( data))\n",
    "\n",
    "        # plot the WordCloud image                      \n",
    "        plt.figure(figsize = fig_size, facecolor = None)\n",
    "        plt.imshow(x)\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(pad = 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e126da4-ae80-4b44-8003-2210ccf0aa75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
